# postman-assignment

### Improvements; What is new?
1. Airflow Orchestration issue with 2.0 fixed [ in latest 2.0 release lot of new features has been introduced (eg TaskFlow APIs)]
2. Disabled cancel uploading button after 100% upload of file [Bug]
    <img width="1178" alt="Screenshot 2021-03-18 at 6 09 20 PM" src="https://user-images.githubusercontent.com/19818454/111627451-1270b880-8815-11eb-9d02-f77f194c5617.png">

3. Aggrigation table getting populated each and very 2 minutes using airflow [ for this use case, on production this can be hourly or daily etc]
4. Created web UI for aggregated table with pagination
      <img width="1187" alt="Screenshot 2021-03-18 at 6 10 36 PM" src="https://user-images.githubusercontent.com/19818454/111627625-4055fd00-8815-11eb-8e00-a777a08d5b2a.png">


### Pending
* airflow containerisation [python 3.8 compactability] [Issue] [trying]
* puckle docker image for arflow not compactable with airflow 2.0 version [Issue]


#### Execution

> ./start_script.sh

Containerisation is not successful yet due to
1. Public Issue in Airflow official Docker Image
2. Used ***puckel/docker-airflow*** for airflow service
3. ***aggregated table*** is implemented by airflow job scheduler [NOT Completed Since bug exists]
4. Multiple Data Ingestion possible - FastAPI server hosted with Browser UI

#### Execution on local device
1. Install dependencies using pip [Python 3.8 recommended]
2. move to app directory
3. run ***uvicorn main:app --reload***
4. Open **http://127.0.0.1:8000**


#### Schema

table name - postman

> **porduct**
> 1. name - String
> 2. sku - String
> 3. description - String
> 4. partition_code - Integer
> 

***NOTE: partition code is the partition key, depending on the value of partition code assinged program generates multiple partitions. partition_factor is 0.3 byd default and can have maximum of 1 [1000 partition tables]***

> **porduct_report**
> 1. name - String
> 2. no. of products - Integer
> 

***NOTE: No need to create tables even for local device execution - only thing needed is create a postgresql server on localhost (default port 5432, user role - postgres, password: postgres); Tables and any number of partitions will be generated by the code itself***

> DDL command are present in the **app/models/product.py** with ***Product class***
> 

#### What is done from ‚ÄúPoints to achieve‚Äù.

1. For Non blocking Ingestion created WebUI using FastAPI<img width="1514" alt="Screenshot 2021-03-14 at 9 49 00 PM" src="https://user-images.githubusercontent.com/19818454/111075733-662a8b80-850f-11eb-9928-6b24100ab563.png">

2. Stores Uploaded files in temp directory and read csv as different chunk (max 1024 records) and process that.<img width="1480" alt="Screenshot 2021-03-14 at 9 49 11 PM" src="https://user-images.githubusercontent.com/19818454/111075821-c0c3e780-850f-11eb-9155-4ebdd44e2748.png">
3. Transform each record in a chunk by adding new column partition_code with a custom string hasing class. <img width="684" alt="Screenshot 2021-03-14 at 10 00 16 PM" src="https://user-images.githubusercontent.com/19818454/111076048-ad654c00-8510-11eb-8f49-f636e2edb3a7.png">
4. When ever try to ingest product data with product existing sku - system updates with other data (large number of partition table really speed up the updation process.
5. All the products are saving in single table with varible partition. ***total count - 466693***
    <img width="1680" alt="Screenshot 2021-03-14 at 10 06 04 PM" src="https://user-images.githubusercontent.com/19818454/111076229-7ba0b500-8511-11eb-8083-fd984fe1fc40.png">

6. Aggrigated Table ( this reports the over all product table with a airflow pipeline )  ***total count - 212751***
  <img width="373" alt="Screenshot 2021-03-14 at 10 08 48 PM" src="https://user-images.githubusercontent.com/19818454/111076317-dcc88880-8511-11eb-9835-5bbfd6239f1d.png">


#### What is not done from ‚ÄúPoints to achieve‚Äù?

1. file is to be processed in 2 mins [ On the present implementation time complexity of ingestion module is O(n^2) since pandas chunk processing, if memory is not concerned it can be implemented in O(n) time complexity. Try to implement with PySpark Jobs, I have much experience in PySpark, but I will üí™]
2. An aggregated table on above rows with `name` and `no. of products` as the columns; [all the healper function are implemeted - airflow docker implementation is pending] Since: airflow offcial docker image has some bug with postgress integration (source: opened in github official repository)

> Dag function are implemented, but stuck on error with dependency [PythonOperator]
> 

[ I can improve time complexity (with spark) and Orchestration with airflow if allowed 3 more days. This repository was created during last 2 days (Starurday and Sunday) #_IAmAQuickLearner_üèãÔ∏è‚Äç‚ôÇÔ∏è ]


